# [006] - Implementar CLI Interativo (chat.py)

## Metadados
- **ID**: 006
- **Grupo**: Fase 2 - ImplementaÃ§Ã£o Core RAG
- **Prioridade**: Alta
- **Complexidade**: MÃ©dia
- **Estimativa**: 1 dia

## DescriÃ§Ã£o
Implementar interface CLI interativa usando Typer que recebe perguntas do usuÃ¡rio, usa search.py para buscar contexto, monta prompt para LLM e exibe respostas. Loop contÃ­nuo atÃ© usuÃ¡rio sair.

## Requisitos

### Requisitos Funcionais
- UC-002: Realizar Consulta SemÃ¢ntica
- RF-013: Interface CLI com Typer
- RF-014: Loop de perguntas e respostas
- RF-015: Integrar busca semÃ¢ntica
- RF-016: Chamar LLM (gpt-5-nano ou gpt-4o-mini)

### Requisitos NÃ£o-Funcionais
- RN-001: Respostas baseadas exclusivamente no contexto
- RN-002: Mensagem padrÃ£o quando sem informaÃ§Ã£o
- RNF-012: CLI intuitivo com prompts claros
- RNF-013: Mensagens de erro descritivas

## Fonte da InformaÃ§Ã£o
- **SeÃ§Ã£o 2.3**: Processo de Consulta SemÃ¢ntica (completo)
- **SeÃ§Ã£o 3.2**: Diagrama - chat.py com CLI (Typer)
- **SeÃ§Ã£o 4.1**: UC-002 - Realizar Consulta SemÃ¢ntica
- **SeÃ§Ã£o 4.1**: UC-003 - Validar Resposta Baseada em Contexto

## Stack NecessÃ¡ria
- **Python**: 3.13.9
- **Typer**: 0.20.0
- **LangChain**: langchain_openai (ChatOpenAI)
- **MÃ³dulos**: src/search.py

## DependÃªncias

### DependÃªncias TÃ©cnicas
- Tarefa 004: IngestÃ£o implementada
- Tarefa 005: Busca implementada
- Documentos no banco

### DependÃªncias de NegÃ³cio
- OpenAI API Key vÃ¡lida

## CritÃ©rios de Aceite

1. [x] `src/chat.py` implementado
2. [x] CLI com Typer funcional
3. [x] Loop de perguntas/respostas
4. [x] IntegraÃ§Ã£o com SemanticSearch
5. [x] Chamada ao LLM OpenAI
6. [x] Prompt com regras estritas de contexto
7. [x] Mensagem padrÃ£o para perguntas fora do contexto
8. [x] Comando para sair (quit/exit/sair)
9. [x] Mensagens claras para usuÃ¡rio
10. [x] Tratamento de erros robusto

## ImplementaÃ§Ã£o Resumida

### Arquivo Principal

**Arquivo**: `src/chat.py`

```python
"""
Interface CLI interativa para consultas ao sistema RAG.

Permite fazer perguntas em linguagem natural e receber respostas
baseadas no conteÃºdo dos documentos ingeridos.
"""

import os
import sys

import typer
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

from src.search import SemanticSearch

load_dotenv()

app = typer.Typer()


SYSTEM_PROMPT = """VocÃª Ã© um assistente que responde perguntas baseado EXCLUSIVAMENTE no contexto fornecido.

REGRAS OBRIGATÃ“RIAS:
1. Responda SOMENTE com base no CONTEXTO fornecido
2. Se a informaÃ§Ã£o NÃƒO estiver explicitamente no CONTEXTO, responda:
   "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."
3. NUNCA invente ou use conhecimento externo
4. NUNCA produza opiniÃµes ou interpretaÃ§Ãµes alÃ©m do que estÃ¡ escrito
5. Seja direto e objetivo na resposta

EXEMPLOS DE PERGUNTAS FORA DO CONTEXTO:
Pergunta: "Qual Ã© a capital da FranÃ§a?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Pergunta: "Quantos clientes temos em 2024?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Pergunta: "VocÃª acha isso bom ou ruim?"
Resposta: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."
"""


def build_prompt(context: str, question: str) -> str:
    """
    Monta prompt completo com contexto e pergunta.
    
    Args:
        context: Contexto recuperado do vectorstore
        question: Pergunta do usuÃ¡rio
        
    Returns:
        Prompt formatado
    """
    return f"""CONTEXTO:
{context}

PERGUNTA DO USUÃRIO:
{question}

RESPONDA A "PERGUNTA DO USUÃRIO":"""


def ask_llm(question: str, context: str) -> str:
    """
    Envia pergunta ao LLM com contexto.
    
    Args:
        question: Pergunta do usuÃ¡rio
        context: Contexto recuperado
        
    Returns:
        Resposta do LLM
    """
    llm_model = os.getenv("LLM_MODEL", "gpt-4o-mini")
    
    llm = ChatOpenAI(
        model=llm_model,
        temperature=0,  # DeterminÃ­stico
    )
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=build_prompt(context, question))
    ]
    
    response = llm.invoke(messages)
    return response.content


@app.command()
def main(
    collection: str = typer.Option("rag_documents", help="ColeÃ§Ã£o no banco"),
):
    """
    Inicia chat interativo com o sistema RAG.
    
    Comandos especiais:
    - quit, exit, sair: Encerra o chat
    
    Exemplo:
        python src/chat.py
    """
    typer.echo("ğŸ¤– Sistema de Busca SemÃ¢ntica")
    typer.echo("=" * 50)
    typer.echo("Digite 'quit', 'exit' ou 'sair' para encerrar\n")
    
    try:
        # Inicializar busca
        searcher = SemanticSearch(collection_name=collection)
        
        while True:
            # Solicitar pergunta
            question = typer.prompt("\nğŸ’¬ FaÃ§a sua pergunta")
            
            # Comandos de saÃ­da
            if question.lower() in ["quit", "exit", "sair"]:
                typer.echo("\nğŸ‘‹ AtÃ© logo!")
                break
            
            # Validar pergunta
            if not question.strip():
                typer.echo("âš ï¸  Pergunta vazia. Tente novamente.")
                continue
            
            try:
                typer.echo("\nğŸ” Buscando informaÃ§Ãµes...")
                
                # Buscar contexto
                context = searcher.get_context(question)
                
                if not context:
                    typer.echo("âš ï¸  Nenhum contexto encontrado no banco de dados.")
                    typer.echo("   Certifique-se de ter ingerido documentos primeiro.")
                    continue
                
                typer.echo("ğŸ’­ Gerando resposta...")
                
                # Perguntar ao LLM
                answer = ask_llm(question, context)
                
                # Exibir resposta
                typer.echo("\nğŸ“ RESPOSTA:")
                typer.echo("-" * 50)
                typer.echo(answer)
                typer.echo("-" * 50)
                
            except Exception as e:
                typer.echo(f"\nâŒ Erro ao processar pergunta: {e}", err=True)
                typer.echo("Tente novamente ou digite 'quit' para sair.")
                
    except KeyboardInterrupt:
        typer.echo("\n\nğŸ‘‹ Interrompido pelo usuÃ¡rio. AtÃ© logo!")
        sys.exit(0)
    except Exception as e:
        typer.echo(f"\nâŒ Erro fatal: {e}", err=True)
        sys.exit(1)


if __name__ == "__main__":
    app()
```

## Testes de Qualidade e Cobertura

### Testes de IntegraÃ§Ã£o

**Arquivo**: `tests/test_chat.py`

```python
"""Testes para mÃ³dulo de chat."""
import pytest
from src.chat import build_prompt, SYSTEM_PROMPT


def test_build_prompt():
    """Testa construÃ§Ã£o do prompt."""
    context = "Contexto de teste"
    question = "Pergunta de teste"
    
    prompt = build_prompt(context, question)
    
    assert "CONTEXTO:" in prompt
    assert context in prompt
    assert "PERGUNTA DO USUÃRIO:" in prompt
    assert question in prompt


def test_system_prompt_has_rules():
    """Testa que prompt do sistema tem regras."""
    assert "REGRAS OBRIGATÃ“RIAS" in SYSTEM_PROMPT
    assert "NUNCA invente" in SYSTEM_PROMPT
    assert "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias" in SYSTEM_PROMPT
```

**CenÃ¡rios de Teste Manual**:

1. **CenÃ¡rio 1: Pergunta dentro do contexto**
   ```
   ğŸ’¬ FaÃ§a sua pergunta: Qual o faturamento da empresa?
   # Expected: Resposta com informaÃ§Ã£o do PDF
   ```

2. **CenÃ¡rio 2: Pergunta fora do contexto**
   ```
   ğŸ’¬ FaÃ§a sua pergunta: Qual a capital da FranÃ§a?
   # Expected: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias..."
   ```

3. **CenÃ¡rio 3: Sair do chat**
   ```
   ğŸ’¬ FaÃ§a sua pergunta: quit
   # Expected: "ğŸ‘‹ AtÃ© logo!"
   ```

## Checklist de FinalizaÃ§Ã£o

- [x] `src/chat.py` implementado
- [x] CLI com Typer
- [x] Loop interativo
- [x] IntegraÃ§Ã£o com SemanticSearch
- [x] Chamada ao LLM
- [x] SYSTEM_PROMPT com regras estritas
- [x] ValidaÃ§Ã£o de contexto
- [x] Comandos de saÃ­da (quit/exit/sair)
- [x] Tratamento de erros
- [x] Mensagens claras
- [x] Testes manuais executados

## Notas Adicionais

### Uso
```bash
# Iniciar chat
python src/chat.py

# Com coleÃ§Ã£o customizada
python src/chat.py --collection meus_docs
```

### Exemplo de InteraÃ§Ã£o
```
ğŸ¤– Sistema de Busca SemÃ¢ntica
==================================================
Digite 'quit', 'exit' ou 'sair' para encerrar

ğŸ’¬ FaÃ§a sua pergunta: Qual o faturamento da empresa SuperTechIABrazil?

ğŸ” Buscando informaÃ§Ãµes...
ğŸ’­ Gerando resposta...

ğŸ“ RESPOSTA:
--------------------------------------------------
O faturamento foi de 10 milhÃµes de reais.
--------------------------------------------------

ğŸ’¬ FaÃ§a sua pergunta: Quantos clientes temos?

ğŸ” Buscando informaÃ§Ãµes...
ğŸ’­ Gerando resposta...

ğŸ“ RESPOSTA:
--------------------------------------------------
NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.
--------------------------------------------------

ğŸ’¬ FaÃ§a sua pergunta: quit

ğŸ‘‹ AtÃ© logo!
```

## ReferÃªncias
- **Typer Documentation**: https://typer.tiangolo.com/
- **ChatOpenAI**: https://python.langchain.com/docs/integrations/chat/openai
