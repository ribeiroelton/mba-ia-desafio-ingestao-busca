# Sistema RAG - Ingest√£o e Busca Sem√¢ntica

![Python](https://img.shields.io/badge/python-3.13.9-blue)
![LangChain](https://img.shields.io/badge/langchain-0.3.27-green)
![PostgreSQL](https://img.shields.io/badge/postgresql-17-blue)
![Coverage](https://img.shields.io/badge/coverage-80%25+-brightgreen)
![License](https://img.shields.io/badge/license-MIT-green)

Sistema de Retrieval Augmented Generation (RAG) para ingest√£o de documentos PDF e consultas sem√¢nticas usando LangChain, OpenAI e PostgreSQL com pgVector. Inclui framework avan√ßado de testes com **LLM-as-a-Judge** para garantir qualidade das respostas.

## üìã √çndice

- [Vis√£o Geral](#-vis√£o-geral)
- [Arquitetura](#Ô∏è-arquitetura)
- [Funcionalidades](#-funcionalidades)
- [Pr√©-requisitos](#-pr√©-requisitos)
- [Instala√ß√£o](#-instala√ß√£o)
- [Uso](#-uso)
  - [Ingest√£o de PDFs](#ingest√£o-de-pdfs)
  - [Chat Interativo](#chat-interativo)
- [Casos de Teste](#-casos-de-teste)
- [Testes](#-testes)
  - [Framework LLM-as-a-Judge](#framework-llm-as-a-judge)
  - [Executando Testes](#executando-testes)
  - [Cobertura de C√≥digo](#cobertura-de-c√≥digo)
- [Troubleshooting](#-troubleshooting)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Regras de Neg√≥cio](#-regras-de-neg√≥cio)
- [Contribuindo](#-contribuindo)
- [Licen√ßa](#-licen√ßa)

## üéØ Vis√£o Geral

Este sistema implementa um pipeline completo de RAG com foco em **qualidade e confiabilidade**:

1. **Ingest√£o**: Processa PDFs, divide em chunks e armazena embeddings no PostgreSQL
2. **Busca Sem√¢ntica**: Encontra os 10 trechos mais relevantes por similaridade
3. **Chat**: Interface CLI que responde perguntas baseado EXCLUSIVAMENTE no contexto recuperado
4. **Avalia√ß√£o de Qualidade**: Framework LLM-as-a-Judge para validar respostas automaticamente

### Principais Caracter√≠sticas

- ‚úÖ Respostas baseadas **exclusivamente** no contexto dos documentos
- ‚úÖ Mensagem padr√£o quando informa√ß√£o n√£o est√° dispon√≠vel
- ‚úÖ Chunking inteligente (1000 chars, overlap 150)
- ‚úÖ Busca por similaridade de cosseno (top k=10)
- ‚úÖ Interface CLI intuitiva
- ‚úÖ **Framework LLM-as-a-Judge** para avalia√ß√£o autom√°tica de qualidade
- ‚úÖ Testes automatizados com cobertura >= 80%
- ‚úÖ Testes de integra√ß√£o com avalia√ß√£o qualitativa de respostas

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ingest.py  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  PostgreSQL  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  search.py  ‚îÇ
‚îÇ (PDFs ‚Üí DB) ‚îÇ      ‚îÇ  + pgVector  ‚îÇ      ‚îÇ (Busca)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                                    ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ  chat.py    ‚îÇ
                                            ‚îÇ  (CLI)      ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                                    ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ OpenAI LLM  ‚îÇ
                                            ‚îÇ (Resposta)  ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                                    ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ LLM Judge   ‚îÇ
                                            ‚îÇ (Avalia√ß√£o) ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes

- **ingest.py**: Carrega PDFs, gera chunks e embeddings, armazena no banco
- **search.py**: Busca sem√¢ntica com k=10 fixo
- **chat.py**: Interface CLI para perguntas e respostas
- **PostgreSQL + pgVector**: Armazenamento de embeddings
- **OpenAI**: Embeddings (text-embedding-3-small) e LLM (gpt-5-nano)
- **LLM-as-a-Judge**: Framework de avalia√ß√£o autom√°tica de qualidade

## üöÄ Funcionalidades

### UC-001: Ingest√£o de Documentos
- Carrega arquivos PDF
- Divide em chunks de 1000 caracteres (overlap 150)
- Gera embeddings com OpenAI
- Armazena no PostgreSQL com pgVector

### UC-002: Consulta Sem√¢ntica
- Busca por similaridade de cosseno
- Retorna top 10 trechos mais relevantes
- Concatena contexto para o LLM

### UC-003: Valida√ß√£o de Contexto
- Respostas baseadas **exclusivamente** no contexto
- Mensagem padr√£o quando informa√ß√£o n√£o dispon√≠vel:
  > "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

### UC-004: Avalia√ß√£o de Qualidade (LLM-as-a-Judge)
- Avalia√ß√£o autom√°tica de respostas usando segundo LLM
- Valida√ß√£o de ader√™ncia ao contexto
- Detec√ß√£o de alucina√ß√µes
- Verifica√ß√£o de seguimento de regras
- An√°lise de clareza e objetividade

## üì¶ Pr√©-requisitos

- **Python**: 3.13.9
- **Docker**: Para PostgreSQL
- **OpenAI API Key**: Para embeddings, LLM e avalia√ß√µes

## üîß Instala√ß√£o

### 1. Clone o Reposit√≥rio

```bash
git clone <repository-url>
cd mba-ia-desafio-ingestao-busca
```

### 2. Configure PostgreSQL com Docker

```bash
docker-compose up -d
```

Isso inicia PostgreSQL 17 com pgVector na porta 5432.

### 3. Configure Ambiente Python

```bash
# Criar ambiente virtual
python3.13 -m venv .venv

# Ativar ambiente
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate     # Windows

# Instalar depend√™ncias
pip install -r requirements.txt
```

### 4. Configure Vari√°veis de Ambiente

Crie arquivo `.env` na raiz:

```bash
# PostgreSQL
CONNECTION_STRING=postgresql+psycopg://postgres:postgres@localhost:5432/rag

# OpenAI
OPENAI_API_KEY=sk-your-key-here

# Modelos (opcional)
EMBEDDING_MODEL=text-embedding-3-small
LLM_MODEL=gpt-5-nano
```

## üéÆ Uso

### Ingest√£o de PDFs

Ingira um ou mais documentos PDF:

```bash
# Ingerir um PDF
python src/ingest.py documento.pdf

# Com cole√ß√£o customizada
python src/ingest.py documento.pdf --collection minha_colecao

# Exemplo real
python src/ingest.py relatorio_financeiro.pdf
```

**Sa√≠da esperada**:
```
üìÑ Processando: relatorio_financeiro.pdf
‚úÖ 15 chunks criados
üíæ Armazenando embeddings no banco...
‚úÖ Ingest√£o conclu√≠da com sucesso!
```

### Chat Interativo

Inicie o chat para fazer perguntas:

```bash
# Chat padr√£o
python src/chat.py

# Com cole√ß√£o espec√≠fica
python src/chat.py --collection minha_colecao
```

**Exemplo de intera√ß√£o**:
```
ü§ñ Sistema de Busca Sem√¢ntica
==================================================
Digite 'quit', 'exit' ou 'sair' para encerrar

üí¨ Fa√ßa sua pergunta: Qual foi o faturamento da empresa?

üîç Buscando informa√ß√µes...
üí≠ Gerando resposta...

üìù RESPOSTA:
--------------------------------------------------
O faturamento da empresa foi de 10 milh√µes de reais em 2024.
--------------------------------------------------

üí¨ Fa√ßa sua pergunta: Qual √© a capital da Fran√ßa?

üîç Buscando informa√ß√µes...
üí≠ Gerando resposta...

üìù RESPOSTA:
--------------------------------------------------
N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta.
--------------------------------------------------

üí¨ Fa√ßa sua pergunta: quit

üëã At√© logo!
```

## üß™ Casos de Teste

### CT-001: Pergunta com Contexto ‚úÖ

**Cen√°rio**: Documento cont√©m "Faturamento foi 10 milh√µes"  
**Pergunta**: "Qual foi o faturamento?"  
**Resposta Esperada**: Informa√ß√£o correta do documento  
**Valida√ß√£o**: LLM-as-a-Judge com score >= 70

### CT-002: Pergunta sem Contexto ‚úÖ

**Cen√°rio**: Documento sobre empresa, pergunta sobre capital de pa√≠s  
**Pergunta**: "Qual √© a capital da Fran√ßa?"  
**Resposta Esperada**: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."  
**Valida√ß√£o**: Score de rule_following >= 90

### CT-003: Informa√ß√£o Parcial ‚úÖ

**Cen√°rio**: Documento tem informa√ß√£o limitada  
**Pergunta**: Requer informa√ß√£o n√£o dispon√≠vel  
**Resposta Esperada**: Resposta com informa√ß√£o dispon√≠vel ou admiss√£o de limita√ß√£o  
**Valida√ß√£o**: Sem alucina√ß√µes (hallucination_detection >= 80)

## üß™ Testes

### Framework LLM-as-a-Judge

Este projeto implementa um **framework avan√ßado de avalia√ß√£o** usando o padr√£o **LLM-as-a-Judge**, onde um segundo LLM avalia objetivamente a qualidade das respostas do sistema principal.

#### Crit√©rios de Avalia√ß√£o

O framework avalia cada resposta em **4 dimens√µes**:

| Crit√©rio | Peso | Descri√ß√£o |
|----------|------|-----------|
| **Ader√™ncia ao Contexto** | 30% | Resposta baseada exclusivamente no contexto fornecido |
| **Detec√ß√£o de Alucina√ß√£o** | 30% | Aus√™ncia de informa√ß√µes inventadas ou inferidas |
| **Seguimento de Regras** | 25% | Ader√™ncia rigorosa ao SYSTEM_PROMPT |
| **Clareza e Objetividade** | 15% | Resposta clara, direta e completa |

#### Estrutura de Avalia√ß√£o

```python
# Exemplo de uso do framework
from tests.utils.llm_evaluator import LLMEvaluator

evaluator = LLMEvaluator(threshold=70)
result = evaluator.evaluate(
    question="Qual o faturamento?",
    context="Faturamento de R$10M",
    response="O faturamento √© de R$10M",
    system_prompt=SYSTEM_PROMPT
)

print(f"Score Geral: {result.overall_score}")
print(f"Passou: {result.passed}")
print(f"Feedback: {result.feedback}")
```

#### Resultado da Avalia√ß√£o

Cada avalia√ß√£o retorna um `EvaluationResult` com:

- **score**: Score geral (0-100)
- **criteria_scores**: Scores individuais por crit√©rio
- **feedback**: An√°lise detalhada em portugu√™s
- **passed**: Boolean indicando se passou no threshold
- **details**: Detalhes adicionais por crit√©rio

#### M√©tricas de Custo

Framework para avalia√ß√£o autom√°tica de qualidade:

- Modelo: gpt-5-nano
- Tokens por avalia√ß√£o: ~600-900
- Custo por avalia√ß√£o: ~$0.00006-0.00009

**Detalhamento por arquivo**:
- `test_llm_quality_evaluation.py`: 4 testes
- `test_real_scenarios.py`: 4 testes
- `test_business_rules.py`: 2 testes t√©cnicos (RN-005, RN-006)
- **Total**: 10 testes de integra√ß√£o + 28 testes unit√°rios = **38 testes**

### Executando Testes

```bash
# Suite completa
pytest

# Somente testes unit√°rios
pytest tests/unit/ -v

# Somente testes de integra√ß√£o
pytest tests/integration/ -v

# Testes de avalia√ß√£o LLM
pytest tests/integration/test_llm_quality_evaluation.py -v

# Testes unit√°rios do framework LLM-as-a-Judge
pytest tests/unit/test_llm_evaluator_unit.py -v

# Com cobertura detalhada
pytest --cov=src --cov-report=html --cov-report=term-missing

# Abrir relat√≥rio HTML
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
```

### Tipos de Testes

#### 1. Testes Unit√°rios (`tests/unit/`)

Validam componentes isoladamente (28 testes):
- **test_chat_validation.py**: Valida√ß√£o de inputs e outputs do chat (4 testes)
- **test_ingest_validation.py**: Valida√ß√£o do processo de ingest√£o (6 testes)
- **test_search_validation.py**: Valida√ß√£o da busca sem√¢ntica (5 testes)
- **test_llm_evaluator_unit.py**: Testes do framework de avalia√ß√£o (13 testes focados)

#### 2. Testes de Integra√ß√£o (`tests/integration/`)

Validam fluxos completos end-to-end (10 testes):
- **test_business_rules.py**: Regras de neg√≥cio t√©cnicas - chunking, k=10 (2 testes)
- **test_real_scenarios.py**: Cen√°rios reais de uso (4 testes)
- **test_llm_quality_evaluation.py**: Avalia√ß√£o qualitativa com LLM-as-a-Judge (4 testes)

**Nota**: Suite otimizada para remover redund√¢ncias - de 16 para 10 testes de integra√ß√£o (37,5% redu√ß√£o) mantendo 100% de cobertura funcional.

#### 3. Testes de Qualidade LLM (`tests/integration/test_llm_quality_evaluation.py`)

Validam **aspectos qualitativos** das respostas:

```python
def test_factual_accuracy_direct_question(quality_test_collection, llm_evaluator):
    """Valida resposta factual para pergunta direta."""
    searcher = SemanticSearch(collection_name=quality_test_collection)
    question = "Qual √© o faturamento da empresa Alfa Energia S.A.?"
    context = searcher.get_context(question)
    response = ask_llm(question, context)
    
    evaluation = llm_evaluator.evaluate(
        question=question,
        context=context,
        response=response,
        system_prompt=SYSTEM_PROMPT
    )
    
    assert evaluation.passed
    assert evaluation.criteria_scores["adherence_to_context"] >= 75
    assert evaluation.criteria_scores["hallucination_detection"] >= 80
```

### Cobertura de C√≥digo

Objetivo: **>= 80% de cobertura**

```bash
# Gerar relat√≥rio de cobertura
pytest --cov=src --cov-report=html --cov-report=term-missing

# Visualizar no terminal
pytest --cov=src --cov-report=term

# Abrir relat√≥rio HTML detalhado
open htmlcov/index.html
```

#### M√©tricas de Cobertura

O relat√≥rio HTML mostra:
- ‚úÖ Cobertura por arquivo
- ‚úÖ Linhas cobertas vs n√£o cobertas
- ‚úÖ Branches cobertos
- ‚úÖ Linhas espec√≠ficas n√£o testadas (destaque vermelho)

### Valida√ß√£o Completa

```bash
# Script de valida√ß√£o autom√°tica (quando dispon√≠vel)
chmod +x scripts/validate.sh
./scripts/validate.sh
```

## üîß Troubleshooting

### Problema: `ModuleNotFoundError: No module named 'langchain'`

**Solu√ß√£o**:
```bash
# Ativar ambiente virtual primeiro
source .venv/bin/activate

# Reinstalar depend√™ncias
pip install -r requirements.txt
```

### Problema: `psycopg.OperationalError: connection refused`

**Solu√ß√£o**:
1. Verifique se PostgreSQL est√° rodando:
   ```bash
   docker ps | grep rag-postgres
   ```
2. Se n√£o estiver, inicie:
   ```bash
   docker-compose up -d
   ```
3. Verifique logs:
   ```bash
   docker logs rag-postgres
   ```

### Problema: `AuthenticationError: Invalid API key`

**Solu√ß√£o**:
1. Verifique se `.env` existe e cont√©m `OPENAI_API_KEY`
2. Valide a key em: https://platform.openai.com/api-keys
3. Certifique-se de que a key tem cr√©ditos dispon√≠veis

### Problema: LLM n√£o segue regras (inventa respostas)

**Solu√ß√£o**:
1. Verificar `SYSTEM_PROMPT` em `src/chat.py`
2. Ajustar temperatura para 0 (determin√≠stico)
3. Testar com modelo mais recente (gpt-4o-mini ou gpt-4)
4. Validar com testes LLM-as-a-Judge:
   ```bash
   pytest tests/integration/test_llm_quality_evaluation.py -v
   ```

### Problema: Busca retorna contexto vazio

**Solu√ß√£o**:
1. Verifique se documentos foram ingeridos:
   ```bash
   docker exec -it rag-postgres psql -U postgres -d rag -c "SELECT COUNT(*) FROM langchain_pg_embedding;"
   ```
2. Se 0, ingira documentos primeiro:
   ```bash
   python src/ingest.py seu_documento.pdf
   ```

### Problema: Testes LLM-as-a-Judge falhando

**Poss√≠veis causas**:
1. **API Key inv√°lida ou sem cr√©ditos**
   ```bash
   # Verificar se API key est√° configurada
   echo $OPENAI_API_KEY
   ```
2. **Threshold muito alto**
   - Ajustar threshold no `LLMEvaluator` (padr√£o: 70)
3. **Modelo inadequado**
   - Usar gpt-4 ou gpt-4o-mini para avalia√ß√µes mais precisas

## üìÅ Estrutura do Projeto

```
mba-ia-desafio-ingestao-busca/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ prompts/
‚îÇ       ‚îî‚îÄ‚îÄ dev-python-rag.prompt.md   # Instru√ß√µes para desenvolvimento
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py                      # Ingest√£o de PDFs
‚îÇ   ‚îú‚îÄ‚îÄ search.py                      # Busca sem√¢ntica
‚îÇ   ‚îî‚îÄ‚îÄ chat.py                        # Interface CLI
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                    # Fixtures compartilhadas
‚îÇ   ‚îú‚îÄ‚îÄ unit/                          # Testes unit√°rios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_chat_validation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_ingest_validation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_search_validation.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_llm_evaluator_unit.py # Framework LLM-as-a-Judge
‚îÇ   ‚îú‚îÄ‚îÄ integration/                   # Testes de integra√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_e2e_core.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_business_rules.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_real_scenarios.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_llm_quality_evaluation.py  # Avalia√ß√£o LLM
‚îÇ   ‚îú‚îÄ‚îÄ utils/                         # Utilit√°rios de teste
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_evaluator.py          # LLM-as-a-Judge framework
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation_criteria.py    # Crit√©rios de avalia√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/                      # Fixtures de teste
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ validate.sh                    # Valida√ß√£o completa
‚îú‚îÄ‚îÄ htmlcov/                           # Relat√≥rio de cobertura
‚îú‚îÄ‚îÄ docker-compose.yaml                # PostgreSQL + pgVector
‚îú‚îÄ‚îÄ init.sql                           # Inicializa√ß√£o do banco
‚îú‚îÄ‚îÄ requirements.txt                   # Depend√™ncias Python
‚îú‚îÄ‚îÄ pytest.ini                         # Configura√ß√£o pytest
‚îú‚îÄ‚îÄ .env                               # Vari√°veis de ambiente
‚îú‚îÄ‚îÄ LICENSE                            # Licen√ßa MIT
‚îî‚îÄ‚îÄ README.md                          # Este arquivo
```

## üìú Regras de Neg√≥cio

| ID | Regra | Descri√ß√£o | Teste |
|----|-------|-----------|-------|
| RN-001 | Contexto Exclusivo | Respostas baseadas SOMENTE no contexto recuperado | `test_business_rules.py` |
| RN-002 | Mensagem Padr√£o | "N√£o tenho informa√ß√µes necess√°rias..." quando sem contexto | `test_llm_quality_evaluation.py` |
| RN-003 | Chunk Size | 1000 caracteres, overlap 150 | `test_ingest_validation.py` |
| RN-004 | Similaridade | Cosine distance | `test_search_validation.py` |
| RN-005 | Embeddings | OpenAI text-embedding-3-small | `test_e2e_core.py` |
| RN-006 | Top K | Fixo em 10 resultados | `test_search_validation.py` |
| RN-007 | Avalia√ß√£o LLM | Score >= 70 para respostas aceit√°veis | `test_llm_quality_evaluation.py` |
| RN-008 | Sem Alucina√ß√£o | Detec√ß√£o de alucina√ß√£o >= 80 | `test_llm_quality_evaluation.py` |

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### Diretrizes

- ‚úÖ Adicione testes para novas funcionalidades
- ‚úÖ Mantenha cobertura >= 80%
- ‚úÖ Siga PEP 8 para estilo de c√≥digo
- ‚úÖ Documente fun√ß√µes e m√≥dulos (Google style)
- ‚úÖ Use type hints em fun√ß√µes p√∫blicas
- ‚úÖ Valide qualidade com LLM-as-a-Judge quando aplic√°vel
- ‚úÖ Execute `pytest` antes de commitar

### Executar Todos os Checks

```bash
# Testes completos
pytest --cov=src --cov-report=html

# Verificar cobertura >= 80%
coverage report --fail-under=80

```

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## üîó Refer√™ncias

### Documenta√ß√£o

- [LangChain](https://python.langchain.com/) - Framework RAG
- [OpenAI API](https://platform.openai.com/docs) - Embeddings e LLM
- [pgVector](https://github.com/pgvector/pgvector) - Busca vetorial no PostgreSQL
- [Typer](https://typer.tiangolo.com/) - CLI framework
- [Pytest](https://docs.pytest.org/) - Framework de testes
- [LLM-as-a-Judge Pattern](https://arxiv.org/abs/2306.05685) - Padr√£o de avalia√ß√£o

### Tutoriais

- [RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/) - LangChain RAG
- [PostgreSQL + pgVector](https://github.com/langchain-ai/langchain-postgres) - Integra√ß√£o
- [Testing LLM Applications](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation) - M√©tricas de avalia√ß√£o
- [LLM-as-a-Judge Best Practices](https://eugeneyan.com/writing/llm-patterns/#llm-as-a-judge) - Boas pr√°ticas

### Artigos e Papers

- [Judging LLM-as-a-Judge with MT-Bench](https://arxiv.org/abs/2306.05685)
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [Vector Database Comparison](https://benchmark.vectorview.ai/)

---

**Desenvolvido como parte do MBA em Intelig√™ncia Artificial**

Para d√∫vidas ou suporte, abra uma issue no reposit√≥rio.

### üéØ Highlights do Projeto

- ‚ú® **Framework LLM-as-a-Judge** propriet√°rio para avalia√ß√£o autom√°tica de qualidade
- ‚ú® **Cobertura >= 80%** com testes unit√°rios e de integra√ß√£o
- ‚ú® **4 dimens√µes de avalia√ß√£o**: Contexto, Alucina√ß√£o, Regras, Clareza
- ‚ú® **Valida√ß√£o autom√°tica** de respostas usando segundo LLM
- ‚ú® **Testes qualitativos** que v√£o al√©m de valida√ß√µes estruturais
- ‚ú® **Pipeline CI/CD ready** com pytest e coverage reports
