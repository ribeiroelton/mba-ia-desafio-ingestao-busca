"""
Framework de avalia√ß√£o LLM usando LLM-as-a-Judge pattern.

Avalia qualidade de respostas LLM usando um segundo LLM avaliador.
"""
import json
import os
from typing import Dict, Optional
from dataclasses import dataclass

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from tests.utils.evaluation_criteria import RagEvaluationCriteria


@dataclass
class EvaluationResult:
    """Resultado de uma avalia√ß√£o LLM."""
    score: int  # 0-100
    criteria_scores: Dict[str, int]  # Scores por crit√©rio
    feedback: str  # Feedback detalhado
    passed: bool  # Se passou no threshold
    details: Dict[str, str]  # Detalhes por crit√©rio
    
    @property
    def overall_score(self) -> int:
        """Alias para score (compatibilidade)."""
        return self.score


class LLMEvaluator:
    """
    Avaliador de qualidade de respostas LLM usando LLM-as-a-Judge pattern.
    
    Usa um segundo LLM para avaliar objetivamente se a resposta:
    - Segue SYSTEM_PROMPT rigorosamente
    - Est√° baseada exclusivamente no contexto
    - N√£o cont√©m alucina√ß√µes
    - √â clara, objetiva e direta
    
    Example:
        >>> evaluator = LLMEvaluator(threshold=70)
        >>> result = evaluator.evaluate(
        ...     question="Qual o faturamento?",
        ...     context="Faturamento de R$10M",
        ...     response="O faturamento √© de R$10M"
        ... )
        >>> assert result.passed
        >>> print(result.score)
        85
    """
    
    EVALUATOR_SYSTEM_PROMPT = """Voc√™ √© um avaliador especializado em validar respostas de sistemas RAG.

MISS√ÉO: Avaliar se a RESPOSTA do assistente segue rigorosamente as regras e est√° baseada exclusivamente no CONTEXTO fornecido.

CRIT√âRIOS DE AVALIA√á√ÉO (cada um vale 0-100 pontos):

1. ADER√äNCIA AO CONTEXTO (30 pontos)
   - Resposta baseada EXCLUSIVAMENTE no contexto fornecido?
   - Nenhuma informa√ß√£o externa ou conhecimento geral usado?
   - Fatos mencionados podem ser rastreados ao contexto?
   
   ‚úì EXEMPLOS DE BOAS RESPOSTAS:
     ‚Ä¢ Resposta cita trecho exato do contexto
     ‚Ä¢ Resposta sintetiza informa√ß√µes presentes no contexto
     ‚Ä¢ Resposta admite falta de informa√ß√£o quando contexto insuficiente
   
   ‚úó EXEMPLOS DE M√ÅS RESPOSTAS:
     ‚Ä¢ Resposta inclui fatos n√£o presentes no contexto
     ‚Ä¢ Resposta usa conhecimento geral externo
     ‚Ä¢ Resposta extrapola al√©m do contexto

2. DETEC√á√ÉO DE ALUCINA√á√ÉO (30 pontos)
   - Resposta N√ÉO inventa ou infere informa√ß√µes?
   - Resposta N√ÉO extrapola al√©m do escrito?
   - Se info n√£o dispon√≠vel, usa mensagem padr√£o?
   
   ‚úì EXEMPLOS DE BOAS RESPOSTAS:
     ‚Ä¢ Resposta afirma apenas fatos rastre√°veis ao contexto
     ‚Ä¢ Resposta n√£o cria n√∫meros ou estat√≠sticas n√£o mencionadas
     ‚Ä¢ Resposta n√£o cria detalhes n√£o mencionados
   
   ‚úó EXEMPLOS DE M√ÅS RESPOSTAS:
     ‚Ä¢ Resposta inventa estat√≠sticas n√£o presentes
     ‚Ä¢ Resposta adiciona nomes ou datas n√£o mencionados
     ‚Ä¢ Resposta infere causas n√£o expl√≠citas

3. SEGUIMENTO DE REGRAS (25 pontos)
   - Segue regra de responder "N√£o tenho informa√ß√µes necess√°rias" quando apropriado?
   - N√ÉO produz opini√µes ou interpreta√ß√µes al√©m do texto?
   - N√ÉO usa conhecimento externo?
   
   ‚úì EXEMPLOS DE BOAS RESPOSTAS:
     ‚Ä¢ Usa mensagem padr√£o quando informa√ß√£o n√£o dispon√≠vel
     ‚Ä¢ N√£o produz opini√µes ou interpreta√ß√µes
     ‚Ä¢ N√£o usa conhecimento externo
   
   ‚úó EXEMPLOS DE M√ÅS RESPOSTAS:
     ‚Ä¢ Responde com conhecimento geral quando contexto insuficiente
     ‚Ä¢ Produz opini√µes pessoais
     ‚Ä¢ Ignora restri√ß√µes do SYSTEM_PROMPT

4. CLAREZA E OBJETIVIDADE (15 pontos)
   - Resposta √© clara e direta?
   - Resposta √© objetiva sem enrola√ß√£o?
   - Resposta responde a pergunta de forma completa?
   
   ‚úì EXEMPLOS DE BOAS RESPOSTAS:
     ‚Ä¢ Resposta √© direta e clara
     ‚Ä¢ Resposta √© objetiva sem enrola√ß√£o
     ‚Ä¢ Resposta responde completamente a pergunta
   
   ‚úó EXEMPLOS DE M√ÅS RESPOSTAS:
     ‚Ä¢ Resposta √© confusa ou amb√≠gua
     ‚Ä¢ Resposta tem enrola√ß√£o desnecess√°ria
     ‚Ä¢ Resposta n√£o responde a pergunta adequadamente

FORMATO DE SA√çDA:
Retorne APENAS um JSON v√°lido com esta estrutura exata:
{
    "adherence_to_context": <0-100>,
    "hallucination_detection": <0-100>,
    "rule_following": <0-100>,
    "clarity_objectivity": <0-100>,
    "overall_score": <m√©dia ponderada>,
    "feedback": "<an√°lise detalhada em portugu√™s>",
    "passed": <true se overall_score >= 70, false caso contr√°rio>
}

IMPORTANTE:
- Seja rigoroso: prefira scores baixos se houver d√∫vida
- Feedback deve explicar EXATAMENTE o que est√° bom e o que est√° ruim
- Se detectar alucina√ß√£o, score de hallucination_detection deve ser 0
- Se usar conhecimento externo, score de adherence_to_context deve ser 0
"""
    
    def __init__(self, threshold: int = 70, model: str = "gpt-5-nano"):
        """
        Inicializa avaliador LLM.
        
        Args:
            threshold: Score m√≠nimo para passar (0-100)
            model: Modelo OpenAI para avalia√ß√£o (padr√£o: gpt-5-nano)
        """
        self.threshold = threshold
        self.model = model
        self.llm = ChatOpenAI(
            model=model,
            temperature=0,  # Determin√≠stico
        )
    
    def evaluate(
        self,
        question: str,
        context: str,
        response: str,
        system_prompt: Optional[str] = None
    ) -> EvaluationResult:
        """
        Avalia qualidade de uma resposta LLM.
        
        Args:
            question: Pergunta do usu√°rio
            context: Contexto fornecido ao LLM
            response: Resposta gerada pelo LLM
            system_prompt: SYSTEM_PROMPT usado (opcional)
            
        Returns:
            EvaluationResult com scores e feedback
            
        Raises:
            ValueError: Se avalia√ß√£o falhar ou JSON inv√°lido
            
        Example:
            >>> evaluator = LLMEvaluator()
            >>> result = evaluator.evaluate(
            ...     question="Quantos funcion√°rios?",
            ...     context="A empresa tem 50 funcion√°rios",
            ...     response="A empresa tem 50 funcion√°rios"
            ... )
            >>> assert result.passed
        """
        # Montar prompt de avalia√ß√£o
        evaluation_prompt = self.build_evaluation_prompt(
            question=question,
            context=context,
            response=response,
            system_prompt=system_prompt
        )
        
        # Enviar para LLM avaliador
        messages = [
            SystemMessage(content=self.EVALUATOR_SYSTEM_PROMPT),
            HumanMessage(content=evaluation_prompt)
        ]
        
        try:
            llm_response = self.llm.invoke(messages)
            response_text = llm_response.content
            
            # Parsear resposta JSON
            evaluation_data = self._parse_evaluation_response(response_text)
            
            # Criar EvaluationResult
            result = self._build_evaluation_result(evaluation_data)
            
            return result
            
        except json.JSONDecodeError as e:
            raise ValueError(
                f"Avaliador retornou JSON inv√°lido: {e}\n"
                f"Resposta raw: {llm_response.content}"
            )
        except Exception as e:
            raise ValueError(f"Erro ao avaliar resposta: {e}")
    
    def build_evaluation_prompt(
        self,
        question: str,
        context: str,
        response: str,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        Monta prompt para avalia√ß√£o.
        
        Args:
            question: Pergunta do usu√°rio
            context: Contexto fornecido
            response: Resposta do LLM
            system_prompt: SYSTEM_PROMPT usado
            
        Returns:
            Prompt formatado para avalia√ß√£o
        """
        prompt_parts = []
        
        if system_prompt:
            prompt_parts.append(f"SYSTEM_PROMPT DO ASSISTENTE:\n{system_prompt}\n")
        
        prompt_parts.extend([
            "CONTEXTO FORNECIDO AO ASSISTENTE:",
            context,
            "",
            "PERGUNTA DO USU√ÅRIO:",
            question,
            "",
            "RESPOSTA DO ASSISTENTE:",
            response,
            "",
            "AVALIE A RESPOSTA DO ASSISTENTE SEGUINDO OS CRIT√âRIOS."
        ])
        
        return "\n".join(prompt_parts)
    
    def _parse_evaluation_response(self, response_text: str) -> Dict:
        """
        Parseia resposta JSON do avaliador.
        
        Args:
            response_text: Texto da resposta do LLM
            
        Returns:
            Dict com dados de avalia√ß√£o
            
        Raises:
            json.JSONDecodeError: Se JSON inv√°lido
        """
        # Tentar extrair JSON do texto (pode ter markdown)
        text = response_text.strip()
        
        # Remover markdown code blocks se presentes
        if text.startswith("```json"):
            text = text[7:]
        elif text.startswith("```"):
            text = text[3:]
        
        if text.endswith("```"):
            text = text[:-3]
        
        text = text.strip()
        
        # Tentar parsear JSON normalmente primeiro
        try:
            return json.loads(text, strict=False)
        except json.JSONDecodeError:
            # Se falhar, tentar corrigir problemas comuns com feedback
            # Escapar aspas dentro do campo feedback
            import re
            # Encontrar o campo feedback e escapar aspas dentro dele
            pattern = r'"feedback":\s*"([^"]*(?:"[^"]*)*)"'
            
            def escape_feedback(match):
                feedback_content = match.group(1)
                # Escapar apenas aspas que n√£o s√£o de fechamento do JSON
                escaped = feedback_content.replace('\n', ' ').replace('\r', '')
                return f'"feedback": "{escaped}"'
            
            text_fixed = re.sub(pattern, escape_feedback, text, flags=re.DOTALL)
            return json.loads(text_fixed, strict=False)
    
    def _build_evaluation_result(self, data: Dict) -> EvaluationResult:
        """
        Constr√≥i EvaluationResult a partir dos dados.
        
        Args:
            data: Dict com dados de avalia√ß√£o
            
        Returns:
            EvaluationResult estruturado
        """
        criteria_scores = {
            "adherence_to_context": data.get("adherence_to_context", 0),
            "hallucination_detection": data.get("hallucination_detection", 0),
            "rule_following": data.get("rule_following", 0),
            "clarity_objectivity": data.get("clarity_objectivity", 0),
        }
        
        # Calcular score ponderado se n√£o fornecido
        overall_score = data.get("overall_score")
        if overall_score is None:
            overall_score = RagEvaluationCriteria.calculate_weighted_score(criteria_scores)
        
        # Determinar se passou
        passed = data.get("passed")
        if passed is None:
            passed = overall_score >= self.threshold
        
        return EvaluationResult(
            score=int(overall_score),
            criteria_scores=criteria_scores,
            feedback=data.get("feedback", ""),
            passed=bool(passed),
            details={}  # Pode ser expandido no futuro
        )
    
    @staticmethod
    def get_failing_criterion_guidance(evaluation_result: EvaluationResult, threshold: int = 70) -> str:
        """
        Retorna guia com exemplos dos crit√©rios que falharam.
        
        √ötil para debug e corre√ß√£o de respostas que falharam na avalia√ß√£o.
        
        Args:
            evaluation_result: Resultado de uma avalia√ß√£o
            threshold: Score m√≠nimo considerado de sucesso (padr√£o: 70)
            
        Returns:
            Texto formatado com exemplos dos crit√©rios que falharam
            
        Example:
            >>> result = evaluator.evaluate(question, context, response)
            >>> if not result.passed:
            ...     guidance = LLMEvaluator.get_failing_criterion_guidance(result)
            ...     print(guidance)
        """
        failing_criteria = []
        
        # Identificar crit√©rios que falharam
        for criterion_name, score in evaluation_result.criteria_scores.items():
            if score < threshold:
                failing_criteria.append((criterion_name, score))
        
        if not failing_criteria:
            return "‚úì Nenhum crit√©rio falhou!"
        
        # Ordenar por score (pior primeiro)
        failing_criteria.sort(key=lambda x: x[1])
        
        lines = []
        lines.append(f"‚ö†Ô∏è CRIT√âRIOS COM FALHA (score < {threshold}):\n")
        
        for criterion_name, score in failing_criteria:
            lines.append(f"{'='*60}")
            lines.append(f"Score: {score}/100\n")
            
            try:
                examples_text = RagEvaluationCriteria.get_criteria_examples_text(criterion_name)
                lines.append(examples_text)
            except ValueError:
                lines.append(f"Crit√©rio '{criterion_name}' n√£o encontrado")
            
            lines.append("")
        
        lines.append(f"{'='*60}")
        lines.append(f"\nüìå Feedback da Avalia√ß√£o:")
        lines.append(evaluation_result.feedback)
        
        return "\n".join(lines)
